{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
      "nltk:  3.3\n",
      "sklearn:  0.19.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "print(\"Python: \", format(sys.version))\n",
    "print(\"nltk: \",format(nltk.__version__))\n",
    "print(\"sklearn: \", format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus -\n",
    "Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.\n",
    "\n",
    "### Lexicon - \n",
    "Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons.\n",
    "    \n",
    "### Token -\n",
    "Each \"entity\" that is a part of whatever was split up based on rules. For example, each word is a token when a sentence is \n",
    "\"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello students!', 'How are you all?', 'Bangladesh Cricket Team have been playing some inspiring cricket.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = 'Hello students! How are you all? Bangladesh Cricket Team have been playing some inspiring cricket.'\n",
    "\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'students', '!', 'How', 'are', 'you', 'all', '?', 'Bangladesh', 'Cricket', 'Team', 'have', 'been', 'playing', 'some', 'inspiring', 'cricket', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"that'll\", 'only', 'own', \"isn't\", 'couldn', 'then', 'has', 'what', 'been', 'and', 'shouldn', 'all', 'at', 'our', \"mightn't\", 'by', 'herself', 'more', \"should've\", 'shan', 'again', \"shan't\", 'didn', 'have', 'wasn', 'once', \"you've\", 'if', 'which', 'during', 'or', 've', 'these', 'is', 'did', \"hasn't\", 'theirs', \"mustn't\", 'it', \"you'll\", 'further', 'are', \"couldn't\", 'because', 'with', \"shouldn't\", \"she's\", 'wouldn', 'had', \"hadn't\", 'who', \"won't\", 'doing', 'up', 'm', 're', 's', 'was', \"don't\", 'to', 'while', 'its', 'just', 'be', 'now', 'o', 'other', \"aren't\", 'from', 'but', 'as', 'don', 'll', 'here', 'ma', 'most', 'the', 'can', 'on', 'having', 'between', 'such', 'both', 'over', 'hers', 'after', 'very', 'weren', 'should', 'yours', 'out', 'not', 'an', 'too', 'before', 'mightn', 'no', 'haven', \"wasn't\", 'i', 'there', 'me', \"doesn't\", 'this', \"you'd\", 'that', 'nor', 'we', 'whom', 'them', 'in', 'he', 'same', 'about', 'itself', 'her', 'ourselves', 'against', 'any', 'd', 'your', 'down', 'some', \"haven't\", 'aren', 'myself', 'how', 'each', 'won', \"didn't\", \"needn't\", 'than', \"it's\", 'where', 'were', 'off', 'of', 'hadn', 'above', 'isn', 'my', 'those', 'below', 'being', 't', 'doesn', 'you', 'through', 'hasn', 'for', \"you're\", 'ours', 'she', 'into', 'his', 'do', 'does', 'their', \"wouldn't\", 'until', 'ain', 'a', 'why', 'themselves', 'yourselves', 'few', 'when', 'him', 'needn', 'mustn', 'will', 'they', 'yourself', 'so', \"weren't\", 'himself', 'am', 'under', 'y'}\n"
     ]
    }
   ],
   "source": [
    "# removing stop words - useless data\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'some', 'sample', 'text', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "example = 'This is some sample text, showing off the stop words filtration.'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "print(word_tokens)\n",
    "print(sentence)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ride\n",
      "ride\n",
      "rider\n",
      "ride\n"
     ]
    }
   ],
   "source": [
    "# Stemming words with NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['ride', 'riding', 'rider', 'riding']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when\n",
      "rider\n",
      "are\n",
      "ride\n",
      "their\n",
      "hors\n",
      ",\n",
      "they\n",
      "often\n",
      "think\n",
      "of\n",
      "how\n",
      "cowboy\n",
      "rode\n",
      "hors\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#Stemming an entire sentence\n",
    "new_text = 'When riders are riding their horses, they often think of how cowboys rode horses.'\n",
    "\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Declaration of Human Rights\n",
      "Preamble\n",
      "Whereas recognition of the inherent dignity and of the equal and inalienable rights of all members of the human family is the foundation of freedom, justice and peace in the world, \n",
      "\n",
      "Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people, \n",
      "\n",
      "Whereas it is essential, if man is not to be compelled to have recourse, as a last resort, to rebellion against tyranny and oppression, that human rights should be protected by the rule of law, \n",
      "\n",
      "Whereas it is essential to promote the development of friendly relations between nations, \n",
      "\n",
      "Whereas the peoples of the United Nations have in the Charter reaffirmed their faith in fundamental human rights, in the dignity and worth of the human person and in the equal rights of men and women and have determined to promote social progress and better standards of life in larger freedom, \n",
      "\n",
      "Whereas Member States have pledged themselves to achieve, in cooperation with the United Nations, the promotion of universal respect for and observance of human rights and fundamental freedoms, \n",
      "\n",
      "Whereas a common understanding of these rights and freedoms is of the greatest importance for the full realization of this pledge, \n",
      "\n",
      "Now, therefore, \n",
      "\n",
      "The General Assembly, \n",
      "\n",
      "Proclaims this Universal Declaration of Human Rights as a common standard of achievement for all peoples and all nations, to the end that every individual and every organ of society, keeping this Declaration constantly in mind, shall strive by teaching and education to promote respect for these rights and freedoms and by progressive measures, national and international, to secure their universal and effective recognition and observance, both among the peoples of Member States themselves and among the peoples of territories under their jurisdiction. \n",
      "\n",
      "Article 1 \n",
      "All human beings are born free and equal in dignity and rights. They are endowed with reason and conscience and should act towards one another in a spirit of brotherhood. \n",
      "\n",
      "Article 2 \n",
      "Everyone is entitled to all the rights and freedoms set forth in this Declaration, without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status. \n",
      "\n",
      "Furthermore, no distinction shall be made on the basis of the political, jurisdictional or international status of the country or territory to which a person belongs, whether it be independent, trust, non-self-governing or under any other limitation of sovereignty. \n",
      "\n",
      "Article 3 \n",
      "Everyone has the right to life, liberty and security of person. \n",
      "\n",
      "Article 4 \n",
      "No one shall be held in slavery or servitude; slavery and the slave trade shall be prohibited in all their forms. \n",
      "\n",
      "Article 5 \n",
      "No one shall be subjected to torture or to cruel, inhuman or degrading treatment or punishment. \n",
      "\n",
      "Article 6 \n",
      "Everyone has the right to recognition everywhere as a person before the law. \n",
      "\n",
      "Article 7 \n",
      "All are equal before the law and are entitled without any discrimination to equal protection of the law. All are entitled to equal protection against any discrimination in violation of this Declaration and against any incitement to such discrimination. \n",
      "\n",
      "Article 8 \n",
      "Everyone has the right to an effective remedy by the competent national tribunals for acts violating the fundamental rights granted him by the constitution or by law. \n",
      "\n",
      "Article 9 \n",
      "No one shall be subjected to arbitrary arrest, detention or exile. \n",
      "\n",
      "Article 10 \n",
      "Everyone is entitled in full equality to a fair and public hearing by an independent and impartial tribunal, in the determination of his rights and obligations and of any criminal charge against him. \n",
      "\n",
      "Article 11 \n",
      "Everyone charged with a penal offence has the right to be presumed innocent until proved guilty according to law in a public trial at which he has had all the guarantees necessary for his defence. \n",
      "No one shall be held guilty of any penal offence on account of any act or omission which did not constitute a penal offence, under national or international law, at the time when it was committed. Nor shall a heavier penalty be imposed than the one that was applicable at the time the penal offence was committed. \n",
      "Article 12 \n",
      "No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks. \n",
      "\n",
      "Article 13 \n",
      "Everyone has the right to freedom of movement and residence within the borders of each State. \n",
      "Everyone has the right to leave any country, including his own, and to return to his country. \n",
      "Article 14 \n",
      "Everyone has the right to seek and to enjoy in other countries asylum from persecution. \n",
      "This right may not be invoked in the case of prosecutions genuinely arising from non-political crimes or from acts contrary to the purposes and principles of the United Nations. \n",
      "Article 15 \n",
      "Everyone has the right to a nationality. \n",
      "No one shall be arbitrarily deprived of his nationality nor denied the right to change his nationality. \n",
      "Article 16 \n",
      "Men and women of full age, without any limitation due to race, nationality or religion, have the right to marry and to found a family. They are entitled to equal rights as to marriage, during marriage and at its dissolution. \n",
      "Marriage shall be entered into only with the free and full consent of the intending spouses. \n",
      "The family is the natural and fundamental group unit of society and is entitled to protection by society and the State. \n",
      "Article 17 \n",
      "Everyone has the right to own property alone as well as in association with others. \n",
      "No one shall be arbitrarily deprived of his property. \n",
      "Article 18 \n",
      "Everyone has the right to freedom of thought, conscience and religion; this right includes freedom to change his religion or belief, and freedom, either alone or in community with others and in public or private, to manifest his religion or belief in teaching, practice, worship and observance. \n",
      "\n",
      "Article 19 \n",
      "Everyone has the right to freedom of opinion and expression; this right includes freedom to hold opinions without interference and to seek, receive and impart information and ideas through any media and regardless of frontiers. \n",
      "\n",
      "Article 20 \n",
      "Everyone has the right to freedom of peaceful assembly and association. \n",
      "No one may be compelled to belong to an association. \n",
      "Article 21 \n",
      "Everyone has the right to take part in the government of his country, directly or through freely chosen representatives. \n",
      "Everyone has the right to equal access to public service in his country. \n",
      "The will of the people shall be the basis of the authority of government; this will shall be expressed in periodic and genuine elections which shall be by universal and equal suffrage and shall be held by secret vote or by equivalent free voting procedures. \n",
      "Article 22 \n",
      "Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality. \n",
      "\n",
      "Article 23 \n",
      "Everyone has the right to work, to free choice of employment, to just and favourable conditions of work and to protection against unemployment. \n",
      "Everyone, without any discrimination, has the right to equal pay for equal work. \n",
      "Everyone who works has the right to just and favourable remuneration ensuring for himself and his family an existence worthy of human dignity, and supplemented, if necessary, by other means of social protection. \n",
      "Everyone has the right to form and to join trade unions for the protection of his interests. \n",
      "Article 24 \n",
      "Everyone has the right to rest and leisure, including reasonable limitation of working hours and periodic holidays with pay. \n",
      "\n",
      "Article 25 \n",
      "Everyone has the right to a standard of living adequate for the health and well-being of himself and of his family, including food, clothing, housing and medical care and necessary social services, and the right to security in the event of unemployment, sickness, disability, widowhood, old age or other lack of livelihood in circumstances beyond his control. \n",
      "Motherhood and childhood are entitled to special care and assistance. All children, whether born in or out of wedlock, shall enjoy the same social protection. \n",
      "Article 26 \n",
      "Everyone has the right to education. Education shall be free, at least in the elementary and fundamental stages. Elementary education shall be compulsory. Technical and professional education shall be made generally available and higher education shall be equally accessible to all on the basis of merit. \n",
      "Education shall be directed to the full development of the human personality and to the strengthening of respect for human rights and fundamental freedoms. It shall promote understanding, tolerance and friendship among all nations, racial or religious groups, and shall further the activities of the United Nations for the maintenance of peace. \n",
      "Parents have a prior right to choose the kind of education that shall be given to their children. \n",
      "Article 27 \n",
      "Everyone has the right freely to participate in the cultural life of the community, to enjoy the arts and to share in scientific advancement and its benefits. \n",
      "Everyone has the right to the protection of the moral and material interests resulting from any scientific, literary or artistic production of which he is the author. \n",
      "Article 28 \n",
      "Everyone is entitled to a social and international order in which the rights and freedoms set forth in this Declaration can be fully realized. \n",
      "\n",
      "Article 29 \n",
      "Everyone has duties to the community in which alone th\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "print(udhr.raw('English-Latin1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have some text, we can train the PunktSentenceTokenizer\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's tokenize the sample text\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Define a function that will tag each tokenized word with a part of speech\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or more repetitions*\n",
    ". = any character except a new line\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking with NLTK\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Tokenizing the sample\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "#Defining a pos_tagging function\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #combine the pos_tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            #draw the chunks with nltk\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
    "<VB.?>* = \"0 or more of any tense of verb\", followed by:\n",
    "<NNP>+ = \"one or more proper nouns\", followed by\n",
    "<NN>? = \"0 or one singular noun.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN)\n",
      "(Chunk Capitol/NNP dome/NN)\n",
      "(Chunk have/VBP served/VBN America/NNP)\n",
      "(Chunk Tonight/NNP)\n",
      "(Chunk Union/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk United/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk September/NNP)\n",
      "(Chunk Dictatorships/NNP shelter/NN)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk Afghanistan/NNP)\n",
      "(Chunk Iraqis/NNP)\n",
      "(Chunk Lebanon/NNP)\n",
      "(Chunk Egypt/NNP)\n",
      "(Chunk Syria/NNP)\n",
      "(Chunk Burma/NNP)\n",
      "(Chunk Zimbabwe/NNP)\n",
      "(Chunk North/NNP Korea/NNP)\n",
      "(Chunk Iran/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP Draper/NNP No/NNP one/NN)\n",
      "(Chunk Islam/NNP)\n",
      "(Chunk Laden/NNP)\n",
      "(Chunk Middle/NNP East/NNP)\n",
      "(Chunk Iraq/NNP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk Beslan/NNP)\n",
      "(Chunk London/NNP)\n",
      "(Chunk Earth/NNP)\n",
      "(Chunk Applause/NNP)\n"
     ]
    }
   ],
   "source": [
    "#Chunking with NLTK\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Tokenizing the sample\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "#Defining a pos_tagging function\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:50]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #combine the pos_tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            #print the nltk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            #draw the chunks with nltk\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk\n",
      "  THE/NNP\n",
      "  UNION/NNP\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  the/DT\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN)\n",
      "(Chunk a/DT)\n",
      "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "(Chunk a/DT noble/JJ dream/NN ./.)\n",
      "(Chunk Tonight/NN we/PRP)\n",
      "(Chunk the/DT hope/NN)\n",
      "(Chunk a/DT glad/JJ reunion/NN)\n",
      "(Chunk the/DT husband/NN who/WP)\n",
      "(Chunk so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
      "(Chunk grateful/JJ)\n",
      "(Chunk the/DT good/JJ life/NN)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk his/PRP$ State/NNP)\n",
      "(Chunk the/DT Union/NNP Address/NNP)\n",
      "(Chunk the/DT Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.)\n",
      "(Chunk 31/CD ,/, 2006/CD ./.)\n",
      "(Chunk White/NNP House/NNP photo/NN)\n",
      "(Chunk Eric/NNP DraperEvery/NNP time/NN I/PRP)\n",
      "(Chunk invited/JJ)\n",
      "(Chunk this/DT rostrum/NN ,/, I/PRP)\n",
      "(Chunk the/DT privilege/NN ,/, and/CC mindful/NN)\n",
      "(Chunk the/DT history/NN we/PRP)\n",
      "(Chunk together/RB ./.)\n",
      "(Chunk We/PRP)\n",
      "(Chunk this/DT Capitol/NNP dome/NN)\n",
      "(Chunk moments/NNS)\n",
      "(Chunk national/JJ mourning/NN and/CC national/JJ achievement/NN ./.)\n",
      "(Chunk We/PRP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk one/CD)\n",
      "(Chunk the/DT most/RBS consequential/JJ periods/NNS)\n",
      "(Chunk our/PRP$ history/NN --/: and/CC it/PRP)\n",
      "(Chunk my/PRP$ honor/NN)\n",
      "(Chunk you/PRP ./.)\n",
      "(Chunk a/DT system/NN)\n",
      "(Chunk\n",
      "  two/CD\n",
      "  parties/NNS\n",
      "  ,/,\n",
      "  two/CD\n",
      "  chambers/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  two/CD\n",
      "  elected/JJ\n",
      "  branches/NNS\n",
      "  ,/,\n",
      "  there/EX\n",
      "  will/MD\n",
      "  always/RB)\n",
      "(Chunk differences/NNS and/CC debate/NN ./.)\n",
      "(Chunk But/CC even/RB tough/JJ debates/NNS can/MD)\n",
      "(Chunk\n",
      "  a/DT\n",
      "  civil/JJ\n",
      "  tone/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  our/PRP$\n",
      "  differences/NNS\n",
      "  can/MD\n",
      "  not/RB)\n",
      "(Chunk anger/NN ./.)\n",
      "(Chunk the/DT great/JJ issues/NNS)\n",
      "(Chunk us/PRP ,/, we/PRP must/MD)\n",
      "(Chunk a/DT spirit/NN)\n",
      "(Chunk goodwill/NN and/CC respect/NN)\n",
      "(Chunk one/CD another/DT --/: and/CC I/PRP will/MD)\n",
      "(Chunk my/PRP$ part/NN ./.)\n",
      "(Chunk Tonight/NNP the/DT state/NN)\n",
      "(Chunk our/PRP$ Union/NNP)\n",
      "(Chunk strong/JJ --/: and/CC together/RB we/PRP will/MD)\n",
      "(Chunk it/PRP stronger/JJR ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n",
      "(Chunk this/DT decisive/JJ year/NN ,/, you/PRP and/CC I/PRP will/MD)\n",
      "(Chunk choices/NNS that/WDT)\n",
      "(Chunk both/DT the/DT future/NN and/CC the/DT character/NN)\n",
      "(Chunk our/PRP$ country/NN ./.)\n",
      "(Chunk We/PRP will/MD)\n",
      "(Chunk confidently/RB)\n",
      "(Chunk the/DT enemies/NNS)\n",
      "(Chunk freedom/NN --/: or/CC retreat/NN)\n",
      "(Chunk our/PRP$ duties/NNS)\n",
      "(Chunk the/DT hope/NN)\n",
      "(Chunk an/DT easier/JJR life/NN ./.)\n",
      "(Chunk We/PRP will/MD)\n",
      "(Chunk our/PRP$ prosperity/NN)\n",
      "(Chunk the/DT world/NN economy/NN --/: or/CC)\n",
      "(Chunk ourselves/PRP off/RP)\n",
      "(Chunk trade/NN and/CC opportunity/NN ./.)\n",
      "(Chunk\n",
      "  a/DT\n",
      "  complex/JJ\n",
      "  and/CC\n",
      "  challenging/JJ\n",
      "  time/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  road/NN)\n",
      "(Chunk isolationism/NN and/CC protectionism/NN may/MD)\n",
      "(Chunk broad/JJ and/CC inviting/NN --/: yet/CC it/PRP)\n",
      "(Chunk danger/NN and/CC decline/NN ./.)\n",
      "(Chunk The/DT only/JJ way/NN)\n",
      "(Chunk our/PRP$ people/NNS ,/, the/DT only/JJ way/NN)\n",
      "(Chunk the/DT peace/NN ,/, the/DT only/JJ way/NN)\n",
      "(Chunk our/PRP$ destiny/NN)\n",
      "(Chunk our/PRP$ leadership/NN --/:)\n",
      "(Chunk the/DT United/NNP States/NNPS)\n",
      "(Chunk America/NNP will/MD)\n",
      "(Chunk ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n"
     ]
    }
   ],
   "source": [
    "#Chinking with nltk\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "#Train the PunktSentenceTokenizer\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "#Tokenizing the sample\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "#Defining a pos_tagging function\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:20]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            #The main difference here is the }{ vs. the{}. This means we're removing\n",
    "            #from the chink one or more verbs, prepositions, determiners, or the word 'to'\n",
    "            \n",
    "            #combine the pos_tag with a regular expression\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                        }<VB.?|IN|TO>+{\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            #print the nltk tree\n",
    "            # print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            #draw the chunks with nltk\n",
    "            #chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entities\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:2]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=False)\n",
    "            \n",
    "            namedEnt.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "## Text Classification using NLTK\n",
    "\n",
    "Now that we have covered the basics of preprocessing for NLP, we can move on to text classification using simple machine\n",
    "learning classification algorithms. \n",
    "\n",
    "Previously covered:\n",
    "\n",
    "Tokenizing\n",
    "\n",
    "Stemming\n",
    "\n",
    "Part-of-Speech tagging\n",
    "\n",
    "Chunking/Chinking\n",
    "\n",
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents:  2000\n",
      "First Review:  (['on', 'the', 'basis', 'of', 'this', 'film', 'alone', ',', 'i', 'never', 'would', 'have', 'predicted', 'that', ',', 'in', 'two', 'years', ',', 'quentin', 'tarantino', 'would', 'become', 'the', 'country', \"'\", 's', 'biggest', 'hotshot', 'director', '.', 'reservoir', 'dogs', 'has', 'hints', 'of', 'the', 'tarantino', 'brilliance', 'that', 'emerged', 'in', 'pulp', 'fiction', ',', 'but', 'is', 'a', 'much', 'less', 'substantive', ',', 'more', 'conventional', 'crime', 'story', 'than', 'the', 'big', 'pf', '.', 'here', \"'\", 's', 'what', 'the', 'two', 'movies', 'do', 'have', 'in', 'common', '.', '.', '.', '--', 'scenes', 'of', 'intelligent', ',', 'amusing', 'dialogue', 'with', 'no', 'relevance', 'to', 'the', 'plot', '--', 'a', 'story', 'that', 'jumps', 'back', 'and', 'forth', 'in', 'time', 'rather', 'than', 'going', 'in', 'a', 'logical', ',', 'chronological', 'sequence', '--', 'graphic', 'yet', 'necessary', 'violence', '--', 'an', 'engaging', 'crime', 'story', '--', 'dialogue', 'liberally', 'spiced', 'with', 'swear', 'words', 'and', 'racial', '/', 'gender', 'slurs', '--', 'three', 'of', 'the', 'same', 'actors', '(', 'harvey', 'keitel', ',', 'tim', 'roth', 'and', 'steve', 'buscemi', ')', '--', 'a', 'mexican', 'standoff', 'ending', ',', 'although', 'this', 'one', 'is', 'considerably', 'less', 'optimistic', 'than', 'pf', 'reservoir', 'dogs', 'is', 'an', 'entire', 'hour', 'shorter', 'than', 'pulp', 'fiction', 'because', 'it', \"'\", 's', 'only', 'got', 'one', 'story', 'to', 'tell', 'rather', 'than', 'three', '.', 'crime', 'lord', 'joe', 'cabot', '(', 'veteran', 'actor', 'lawrence', 'tierney', ')', 'has', 'assembled', 'five', 'criminals', 'who', 'have', 'never', 'even', 'met', 'each', 'other', 'to', 'pull', 'a', 'diamond', 'store', 'heist', '.', 'the', 'five', 'are', 'instructed', 'not', 'to', 'reveal', 'their', 'real', 'names', 'or', 'personal', 'details', 'or', 'anything', 'the', 'cops', 'could', 'use', 'if', 'one', 'of', 'them', 'was', 'captured', 'and', 'interrogated', '.', 'instead', ',', 'they', 'are', 'all', 'given', 'code', 'names', 'off', 'the', 'color', 'chart', '--', 'mr', '.', 'white', '(', 'keitel', ')', ',', 'mr', '.', 'pink', '(', 'buscemi', ')', ',', 'mr', '.', 'orange', '(', 'roth', ')', ',', 'mr', '.', 'blonde', '(', 'michael', 'madsen', ')', ',', 'mr', '.', 'brown', '(', 'tarantino', ')', 'and', 'mr', '.', 'peach', '(', 'tarantino', \"'\", 's', 'chin', ')', '.', 'most', 'of', 'the', 'movie', \"'\", 's', 'running', 'time', 'is', 'spent', 'with', 'mr', '.', 'white', 'and', 'pink', 'waiting', 'at', 'a', 'warehouse', 'for', 'the', 'others', 'to', 'arrive', '.', 'mr', '.', 'orange', ',', 'meanwhile', ',', 'is', 'on', 'the', 'floor', ',', 'dying', 'from', 'a', 'gunshot', 'wound', '.', 'the', 'police', 'had', 'arrived', 'at', 'the', 'scene', 'of', 'the', 'crime', 'way', 'too', 'soon', ',', 'leading', 'white', 'to', 'believe', 'one', 'of', 'the', 'five', 'was', 'an', 'informant', '.', 'the', 'story', \"'\", 's', 'background', 'unfolds', 'through', 'a', 'series', 'of', 'flashbacks', ',', 'of', 'the', 'crime', 'itself', 'and', 'the', 'meetings', 'between', 'joe', 'and', 'the', 'criminals', 'prior', 'to', 'the', 'crime', '.', 'eventually', ',', 'blonde', 'arrives', 'with', 'a', 'police', 'officer', 'hostage', ',', 'and', 'what', 'follows', 'is', 'a', 'truly', 'brutal', ',', 'uncomfortable', 'torture', 'scene', '.', 'let', \"'\", 's', 'just', 'say', 'picasso', 'would', 'be', 'inspired', 'by', 'what', 'blonde', 'does', 'to', 'the', 'cop', \"'\", 's', 'ear', '.', 'a', 'lot', 'of', 'the', 'time', ',', 'reservoir', 'dogs', 'seems', 'like', 'your', 'typical', 'gangster', 'heist', '-', 'gone', '-', 'wrong', 'movie', ',', 'but', 'there', 'are', 'a', 'few', 'sequences', 'that', 'are', 'uniquely', 'tarantino', '.', 'the', 'opening', 'scene', 'in', 'the', 'coffee', 'shop', 'starts', 'the', 'movie', 'off', 'on', 'a', 'high', 'note', 'the', 'rest', 'of', 'it', 'doesn', \"'\", 't', 'live', 'up', 'to', ',', 'as', 'the', 'criminals', 'plus', 'joe', 'have', 'a', 'conversation', 'on', ',', 'among', 'other', 'things', ',', 'tipping', 'philosophies', 'and', 'their', 'interpretations', 'of', 'madonna', \"'\", 's', '\"', 'like', 'a', 'virgin', '.', '\"', 'another', 'scene', 'involves', 'roth', \"'\", 's', 'lengthy', 'manufactured', 'story', 'about', 'running', 'into', 'a', 'group', 'of', 'cops', 'and', 'a', 'police', 'dog', 'in', 'a', 'bus', 'station', 'bathroom', 'while', 'carrying', 'a', 'giant', 'bag', 'of', 'marijuana', '.', 'neither', 'of', 'these', 'have', 'much', 'plot', 'relevance', 'but', 'are', 'given', 'ample', 'time', 'to', 'develop', ',', 'like', 'the', 'captain', 'koons', 'gold', 'watch', 'speech', 'in', 'pulp', 'fiction', ',', 'only', 'here', 'these', 'sequences', 'are', 'more', 'enjoyable', 'than', 'the', 'rest', 'of', 'the', 'movie', '.', 'reservoir', 'dogs', 'is', 'good', 'but', 'it', \"'\", 's', 'no', 'pulp', 'fiction', '.'], 'pos')\n",
      "Most common words:  [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "The word happy:  215\n"
     ]
    }
   ],
   "source": [
    "#build list of documents\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "#shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "print(\"Number of Documents: \", format(len(documents)))\n",
    "print(\"First Review: \", format(documents[0]))\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "print(\"Most common words: \", format(all_words.most_common(15)))\n",
    "print(\"The word happy: \", format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use the 4000 most common words as features\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot\n",
      ":\n",
      "two\n",
      "teen\n",
      "couples\n",
      "go\n",
      "to\n",
      "a\n",
      "church\n",
      "party\n",
      ",\n",
      "drink\n",
      "and\n",
      "then\n",
      "drive\n",
      ".\n",
      "they\n",
      "get\n",
      "into\n",
      "an\n",
      "accident\n",
      "one\n",
      "of\n",
      "the\n",
      "guys\n",
      "dies\n",
      "but\n",
      "his\n",
      "girlfriend\n",
      "continues\n",
      "see\n",
      "him\n",
      "in\n",
      "her\n",
      "life\n",
      "has\n",
      "nightmares\n",
      "what\n",
      "'\n",
      "s\n",
      "deal\n",
      "?\n",
      "watch\n",
      "movie\n",
      "\"\n",
      "sorta\n",
      "find\n",
      "out\n",
      "critique\n",
      "mind\n",
      "-\n",
      "fuck\n",
      "for\n",
      "generation\n",
      "that\n",
      "touches\n",
      "on\n",
      "very\n",
      "cool\n",
      "idea\n",
      "presents\n",
      "it\n",
      "bad\n",
      "package\n",
      "which\n",
      "is\n",
      "makes\n",
      "this\n",
      "review\n",
      "even\n",
      "harder\n",
      "write\n",
      "since\n",
      "i\n",
      "generally\n",
      "applaud\n",
      "films\n",
      "attempt\n",
      "break\n",
      "mold\n",
      "mess\n",
      "with\n",
      "your\n",
      "head\n",
      "such\n",
      "(\n",
      "lost\n",
      "highway\n",
      "&\n",
      "memento\n",
      ")\n",
      "there\n",
      "are\n",
      "good\n",
      "ways\n",
      "making\n",
      "all\n",
      "types\n",
      "these\n",
      "folks\n",
      "just\n",
      "didn\n",
      "t\n",
      "snag\n",
      "correctly\n",
      "seem\n",
      "have\n",
      "taken\n",
      "pretty\n",
      "neat\n",
      "concept\n",
      "executed\n",
      "terribly\n",
      "so\n",
      "problems\n",
      "well\n",
      "its\n",
      "main\n",
      "problem\n",
      "simply\n",
      "too\n",
      "jumbled\n",
      "starts\n",
      "off\n",
      "normal\n",
      "downshifts\n",
      "fantasy\n",
      "world\n",
      "you\n",
      "as\n",
      "audience\n",
      "member\n",
      "no\n",
      "going\n",
      "dreams\n",
      "characters\n",
      "coming\n",
      "back\n",
      "from\n",
      "dead\n",
      "others\n",
      "who\n",
      "look\n",
      "like\n",
      "strange\n",
      "apparitions\n",
      "disappearances\n",
      "looooot\n",
      "chase\n",
      "scenes\n",
      "tons\n",
      "weird\n",
      "things\n",
      "happen\n",
      "most\n",
      "not\n",
      "explained\n",
      "now\n",
      "personally\n",
      "don\n",
      "trying\n",
      "unravel\n",
      "film\n",
      "every\n",
      "when\n",
      "does\n",
      "give\n",
      "me\n",
      "same\n",
      "clue\n",
      "over\n",
      "again\n",
      "kind\n",
      "fed\n",
      "up\n",
      "after\n",
      "while\n",
      "biggest\n",
      "obviously\n",
      "got\n",
      "big\n",
      "secret\n",
      "hide\n",
      "seems\n",
      "want\n",
      "completely\n",
      "until\n",
      "final\n",
      "five\n",
      "minutes\n",
      "do\n",
      "make\n",
      "entertaining\n",
      "thrilling\n",
      "or\n",
      "engaging\n",
      "meantime\n",
      "really\n",
      "sad\n",
      "part\n",
      "arrow\n",
      "both\n",
      "dig\n",
      "flicks\n",
      "we\n",
      "actually\n",
      "figured\n",
      "by\n",
      "half\n",
      "way\n",
      "point\n",
      "strangeness\n",
      "did\n",
      "start\n",
      "little\n",
      "bit\n",
      "sense\n",
      "still\n",
      "more\n",
      "guess\n",
      "bottom\n",
      "line\n",
      "movies\n",
      "should\n",
      "always\n",
      "sure\n",
      "before\n",
      "given\n",
      "password\n",
      "enter\n",
      "understanding\n",
      "mean\n",
      "showing\n",
      "melissa\n",
      "sagemiller\n",
      "running\n",
      "away\n",
      "visions\n",
      "about\n",
      "20\n",
      "throughout\n",
      "plain\n",
      "lazy\n",
      "!\n",
      "okay\n",
      "people\n",
      "chasing\n",
      "know\n",
      "need\n",
      "how\n",
      "giving\n",
      "us\n",
      "different\n",
      "offering\n",
      "further\n",
      "insight\n",
      "down\n",
      "apparently\n",
      "studio\n",
      "took\n",
      "director\n",
      "chopped\n",
      "themselves\n",
      "shows\n",
      "might\n",
      "ve\n",
      "been\n",
      "decent\n",
      "here\n",
      "somewhere\n",
      "suits\n",
      "decided\n",
      "turning\n",
      "music\n",
      "video\n",
      "edge\n",
      "would\n",
      "actors\n",
      "although\n",
      "wes\n",
      "bentley\n",
      "seemed\n",
      "be\n",
      "playing\n",
      "exact\n",
      "character\n",
      "he\n",
      "american\n",
      "beauty\n",
      "only\n",
      "new\n",
      "neighborhood\n",
      "my\n",
      "kudos\n",
      "holds\n",
      "own\n",
      "entire\n",
      "feeling\n",
      "unraveling\n",
      "overall\n",
      "doesn\n",
      "stick\n",
      "because\n",
      "entertain\n",
      "confusing\n",
      "rarely\n",
      "excites\n",
      "feels\n",
      "redundant\n",
      "runtime\n",
      "despite\n",
      "ending\n",
      "explanation\n",
      "craziness\n",
      "came\n",
      "oh\n",
      "horror\n",
      "slasher\n",
      "flick\n",
      "packaged\n",
      "someone\n",
      "assuming\n",
      "genre\n",
      "hot\n",
      "kids\n",
      "also\n",
      "wrapped\n",
      "production\n",
      "years\n",
      "ago\n",
      "sitting\n",
      "shelves\n",
      "ever\n",
      "whatever\n",
      "skip\n",
      "where\n",
      "joblo\n",
      "nightmare\n",
      "elm\n",
      "street\n",
      "3\n",
      "7\n",
      "/\n",
      "10\n",
      "blair\n",
      "witch\n",
      "2\n",
      "crow\n",
      "9\n",
      "salvation\n",
      "4\n",
      "stir\n",
      "echoes\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "#Build a find_features function that will determine which of the 4000 word features are contained in a review\n",
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#let's use an example of a negative review\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets do it for all the documents\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "#define a seed for reproducibility\n",
    "seed = 1\n",
    "\n",
    "#split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnClassifier(SVC(kernel = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on the training data\n",
    "model.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy:  0.782\n"
     ]
    }
   ],
   "source": [
    "#test on the testing datasets\n",
    "accuracy = nltk.classify.accuracy(model, testing)\n",
    "print(\"SVC Accuracy: \", format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
